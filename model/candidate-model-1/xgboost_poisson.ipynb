{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lon_bin  lat_bin        date  unique_event_count  date_year  tavg  tmin  \\\n",
      "0   -96.45    32.45  2024-04-25                   0       2024  22.2  19.0   \n",
      "1   -96.45    32.45  2024-04-26                   0       2024  21.6  18.3   \n",
      "2   -96.45    32.45  2024-04-27                   0       2024  24.2  19.1   \n",
      "3   -96.45    32.45  2024-04-28                   0       2024  21.0  16.7   \n",
      "4   -96.45    32.45  2024-04-29                   0       2024  20.7  14.4   \n",
      "\n",
      "   tmax  prcp  snow   wdir  wspd  wpgt    pres  tsun  \\\n",
      "0  24.8   1.5   NaN  151.0  13.1   NaN  1014.4   NaN   \n",
      "1  23.5  37.1   NaN  163.0  18.0   NaN  1008.3   NaN   \n",
      "2  28.3  36.1   NaN  150.0  22.1   NaN  1009.0   NaN   \n",
      "3  24.1  65.0   NaN  156.0  16.1   NaN  1010.2   NaN   \n",
      "4  27.5   0.0   NaN   18.0   2.4   NaN  1012.3   NaN   \n",
      "\n",
      "                                    Statistical_Area  Unemployment_Rate(%)  \\\n",
      "0  Dallas-Fort Worth-Arlington, TX Metropolitan S...                   3.9   \n",
      "1  Dallas-Fort Worth-Arlington, TX Metropolitan S...                   3.9   \n",
      "2  Dallas-Fort Worth-Arlington, TX Metropolitan S...                   3.9   \n",
      "3  Dallas-Fort Worth-Arlington, TX Metropolitan S...                   3.9   \n",
      "4  Dallas-Fort Worth-Arlington, TX Metropolitan S...                   3.9   \n",
      "\n",
      "   Unemployment_Rank  CPI(Annual)  \n",
      "0                226          0.0  \n",
      "1                226          0.0  \n",
      "2                226          0.0  \n",
      "3                226          0.0  \n",
      "4                226          0.0  \n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "final_result = pd.read_csv('/Users/pei/Desktop/Pei/Research/RA role in University/utd_prof_ding/processed_incident_count_005.csv')\n",
    "\n",
    "# Define the folder path containing the CSV files\n",
    "folder_path = '/Users/pei/Desktop/Pei/Research/RA role in University/utd_prof_ding/zDPD_data_005_all'  # Change this to your folder path\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Load each CSV file into a DataFrame and append to the list\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.concat([df, pd.read_csv(file_path).iloc[:,1:]])\n",
    "\n",
    "# Display combined DataFrame (optional)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon_bin</th>\n",
       "      <th>lat_bin</th>\n",
       "      <th>date</th>\n",
       "      <th>unique_event_count_drop</th>\n",
       "      <th>date_year</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>...</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>Statistical_Area</th>\n",
       "      <th>Unemployment_Rate(%)</th>\n",
       "      <th>Unemployment_Rank</th>\n",
       "      <th>CPI(Annual)</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>unique_event_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-96.45</td>\n",
       "      <td>32.45</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>22.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dallas-Fort Worth-Arlington, TX Metropolitan S...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lon_bin  lat_bin        date  unique_event_count_drop  date_year  tavg  \\\n",
       "0   -96.45    32.45  2024-04-25                        0       2024  22.2   \n",
       "\n",
       "   tmin  tmax  prcp  snow  ...  wspd  wpgt    pres  tsun  \\\n",
       "0  19.0  24.8   1.5   NaN  ...  13.1   NaN  1014.4   NaN   \n",
       "\n",
       "                                    Statistical_Area Unemployment_Rate(%)  \\\n",
       "0  Dallas-Fort Worth-Arlington, TX Metropolitan S...                  3.9   \n",
       "\n",
       "   Unemployment_Rank  CPI(Annual)  Unnamed: 0  unique_event_count  \n",
       "0                226          0.0      210000                   0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lon_bin'] = df['lon_bin'].round(2)\n",
    "df['lat_bin'] = df['lat_bin'].round(2)\n",
    "df = df.merge(final_result, how = \"left\", \n",
    "              left_on = ['lon_bin', 'lat_bin', 'date'], right_on = ['lon_bin', 'lat_bin', 'date'],\n",
    "              suffixes=['_drop', ''])\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, 11,  4,  7,  6,  5,  8, 10,  3,  9,  2, 12, 41, 31, 21, 24,\n",
       "       27, 28, 42, 30, 23, 36, 26, 29, 38, 35, 25, 34, 19, 14, 39, 17, 33,\n",
       "       32, 22, 37, 46, 20, 13, 15, 45, 49, 51, 40, 16, 44, 43, 48, 47, 52,\n",
       "       50, 55, 92, 67, 68, 18, 75, 65])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.unique_event_count.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Ensure the data is sorted by date\n",
    "df = df.sort_values(by='date')\n",
    "# Assuming df['date'] contains datetime objects\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract year, day of year, week of year, and month-day components\n",
    "df['date_year'] = df['date'].dt.year\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "# Encode day_of_year cyclically\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Encode week_of_year cyclically\n",
    "df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9202952420031484\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['lon_bin', 'lat_bin', 'date_year', 'day_of_year_sin', 'day_of_year_cos', 'week_of_year_sin', 'week_of_year_cos',\n",
    "            'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'Unemployment_Rate(%)', 'Unemployment_Rank', 'CPI(Annual)']\n",
    "target = 'unique_event_count'\n",
    "\n",
    "# Train-test split\n",
    "train_data = df[df['date'] < '2024-01-01']  # Train on data before 2023\n",
    "valid_data = df[(df['date'] >= '2024-01-01') & (df['date'] < '2024-03-01')]  # First two months of 2024 for validation\n",
    "test_data = df[df['date'] >= '2024-03-01']  # Test on data from 2023 onward\n",
    "\n",
    "# # Convert training data to DMatrix for XGBoost\n",
    "# dtrain = xgb.DMatrix(train_data[features], label=train_data[target])\n",
    "\n",
    "# poisson_obj for event count prediction assuming the count follow poisson distribution\n",
    "def poisson_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.exp(preds)  # Ensure predictions are positive\n",
    "    grad = preds - labels  # Gradient: difference between prediction and true count\n",
    "    hess = preds           # Hessian: prediction values (positive)\n",
    "    return grad, hess\n",
    "\n",
    "# Function to perform rolling validation\n",
    "def rolling_validation(train_data, validation_data, params, features, target, num_boost_round=100):\n",
    "    rolling_train_data = train_data.copy()  # Start with training data\n",
    "    validation_dates = sorted(validation_data['date'].unique())  # Unique validation dates\n",
    "    errors = []  # To store RMSE for each validation step\n",
    "    predictions = {}\n",
    "    true_values = {}\n",
    "\n",
    "    for validation_date in validation_dates:\n",
    "        # Get validation data for the current date\n",
    "        validation_day = validation_data[validation_data['date'] == validation_date]\n",
    "        dtrain = xgb.DMatrix(rolling_train_data[features], label=rolling_train_data[target])\n",
    "        dvalidation = xgb.DMatrix(validation_day[features], label=validation_day[target])\n",
    "        \n",
    "        # Train the model\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            verbose_eval=False,\n",
    "            obj=poisson_obj\n",
    "        )\n",
    "        \n",
    "        # Predict for the current validation day\n",
    "        y_validation = validation_day[target].values\n",
    "        y_pred = model.predict(dvalidation)\n",
    "        y_pred_exp = np.exp(y_pred)  # Back-transform predictions\n",
    "        y_pred_rounded = np.clip(np.round(y_pred_exp).astype(int), 0, None)  # Ensure non-negative integers\n",
    "        \n",
    "        # Calculate RMSE for the current day\n",
    "        rmse = np.sqrt(np.mean((y_validation - y_pred_rounded) ** 2))\n",
    "        errors.append(rmse)\n",
    "        \n",
    "        # Store predictions and true values\n",
    "        predictions[validation_date] = y_pred_rounded\n",
    "        true_values[validation_date] = y_validation\n",
    "        \n",
    "        # Incrementally update the training data with the validation day's data\n",
    "        rolling_train_data = pd.concat([rolling_train_data, validation_day])\n",
    "    \n",
    "    # Return average RMSE, predictions, and true values\n",
    "    return np.mean(errors), predictions, true_values, model\n",
    "\n",
    "# Suggested coarse parameter grid for initial experimentation\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 12],  # Include deeper trees for initial exploration\n",
    "    'eta': [0.01, 0.1, 0.3],  # Learning rate\n",
    "    'scale_pos_weight': [1, 10, 50],  # Class imbalance\n",
    "    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
    "    'min_child_weight': [1, 5, 10],  # Minimum sum of weights for child nodes\n",
    "    'objective': ['reg:squarederror'],  # Regression task\n",
    "    'eval_metric': ['rmse']  # Evaluation metric\n",
    "}\n",
    "\n",
    "# Perform grid search with rolling validation on validation data\n",
    "best_rmse = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    avg_rmse, _, _ = rolling_validation(train_data, valid_data, params, features, target, num_boost_round=100)\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation RMSE: {best_rmse}\")\n",
    "\n",
    "# Hyperparameter grid search with rolling validation\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 12],  # Controls the depth of the trees; coarser steps\n",
    "    'eta': [0.01, 0.1, 0.3],  # Learning rate; coarser steps\n",
    "    'scale_pos_weight': [1, 10, 50],  # To handle class imbalance, broader range\n",
    "    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used per tree\n",
    "    'min_child_weight': [1, 5, 10],  # Minimum sum of weights needed to create a child node\n",
    "    'objective': ['reg:squarederror'],  # For regression tasks\n",
    "    'eval_metric': ['rmse']  # Evaluation metric\n",
    "}\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_params = None\n",
    "params_search_record = {}\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # Perform rolling validation for the current parameter combination\n",
    "    avg_rmse, _, _ = rolling_validation(train_data, valid_data, params, features, target, num_boost_round=100)\n",
    "    params_search_record[tuple(params)] = avg_rmse\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation RMSE: {best_rmse}\")\n",
    "\n",
    "# Combine train and validation data for final testing\n",
    "combined_train_valid = pd.concat([train_data, valid_data])\n",
    "\n",
    "# Use the rolling_validation function on the test set\n",
    "test_rmse, test_predictions, test_true_values, latest_model  = rolling_validation(\n",
    "    combined_train_valid, test_data, best_params, features, target, num_boost_round=100\n",
    ")\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "# # Train initial model\n",
    "# combined_train_valid = pd.concat([train_data, valid_data])\n",
    "# dtrain = xgb.DMatrix(combined_train_valid[features], label=combined_train_valid[target])\n",
    "# final_model = xgb.train(best_params, dtrain, num_boost_round=100, obj=poisson_obj)\n",
    "\n",
    "\n",
    "# # Rolling evaluation on the test set with one-step forecasting\n",
    "# rolling_train_data = pd.concat([train_data, valid_data])  # Combine train and validation data\n",
    "# test_dates = sorted(test_data['date'].unique())  # Get unique test dates\n",
    "# errors = []  # To store RMSE for each test day\n",
    "# predictions = {}\n",
    "# true_values = {}\n",
    "\n",
    "# # Train the initial model with the best parameters\n",
    "# dtrain = xgb.DMatrix(rolling_train_data[features], label=rolling_train_data[target])\n",
    "# model = xgb.train(best_params, dtrain, num_boost_round=100, obj=poisson_obj)\n",
    "\n",
    "# for test_date in test_dates:\n",
    "#     # Get test data for the current date\n",
    "#     test_day = test_data[test_data['date'] == test_date]\n",
    "#     dtest = xgb.DMatrix(test_day[features])\n",
    "#     y_test = test_day[target].values  # True values for the current day\n",
    "    \n",
    "#     # Predict for the test day's data\n",
    "#     y_pred = model.predict(dtest)\n",
    "#     y_pred_exp = np.exp(y_pred)  # Back-transform predictions\n",
    "#     y_pred_rounded = np.clip(np.round(y_pred_exp).astype(int), 0, None)  # Ensure non-negative integers\n",
    "    \n",
    "#     # Calculate RMSE for the current day\n",
    "#     rmse = np.sqrt(np.mean((y_test - y_pred_rounded) ** 2))\n",
    "#     errors.append(rmse)\n",
    "    \n",
    "#     # Store predictions and true values\n",
    "#     predictions[test_date.strftime('%Y-%m-%d')] = y_pred_rounded\n",
    "#     true_values[test_date.strftime('%Y-%m-%d')] = y_test\n",
    "    \n",
    "#     # Incrementally update the model with the test day's data\n",
    "#     rolling_train_data = pd.concat([rolling_train_data, test_day])\n",
    "#     dtrain = xgb.DMatrix(rolling_train_data[features], label=rolling_train_data[target])\n",
    "#     model = xgb.train(best_params, dtrain, num_boost_round=100, obj=poisson_obj)\n",
    "\n",
    "# # Overall evaluation\n",
    "# overall_rmse = np.mean(errors)\n",
    "# print(f\"Overall RMSE on Test Data: {overall_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/crime_prediction/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [12:18:48] WARNING: /var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_b6qk1lz_ug/croot/xgboost-split_1724073748391/work/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "latest_model.save_model('model_20240429.xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 28206,\n",
       "         1: 497,\n",
       "         2: 329,\n",
       "         3: 214,\n",
       "         4: 192,\n",
       "         5: 176,\n",
       "         6: 168,\n",
       "         7: 130,\n",
       "         8: 128,\n",
       "         9: 106,\n",
       "         10: 78,\n",
       "         11: 76,\n",
       "         12: 65,\n",
       "         13: 61,\n",
       "         14: 46,\n",
       "         16: 28,\n",
       "         17: 26,\n",
       "         18: 25,\n",
       "         15: 23,\n",
       "         20: 22,\n",
       "         21: 18,\n",
       "         19: 16,\n",
       "         26: 13,\n",
       "         22: 12,\n",
       "         24: 7,\n",
       "         25: 7,\n",
       "         29: 6,\n",
       "         23: 6,\n",
       "         28: 5,\n",
       "         31: 4,\n",
       "         27: 3,\n",
       "         30: 3,\n",
       "         37: 3,\n",
       "         39: 3,\n",
       "         32: 3,\n",
       "         42: 2,\n",
       "         41: 2,\n",
       "         33: 2,\n",
       "         34: 2,\n",
       "         35: 2,\n",
       "         36: 1,\n",
       "         40: 1,\n",
       "         45: 1,\n",
       "         75: 1,\n",
       "         44: 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for arr in test_true_values.values():\n",
    "    element_counts = Counter(arr)\n",
    "    for key in element_counts.keys():\n",
    "        counter[key] = counter.get(key, 0) + element_counts[key]\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 27886,\n",
       "         1: 700,\n",
       "         2: 316,\n",
       "         7: 305,\n",
       "         3: 266,\n",
       "         8: 238,\n",
       "         6: 172,\n",
       "         4: 150,\n",
       "         5: 104,\n",
       "         9: 99,\n",
       "         10: 60,\n",
       "         15: 52,\n",
       "         14: 47,\n",
       "         19: 40,\n",
       "         12: 40,\n",
       "         16: 40,\n",
       "         18: 27,\n",
       "         17: 26,\n",
       "         11: 26,\n",
       "         20: 24,\n",
       "         13: 23,\n",
       "         29: 12,\n",
       "         27: 9,\n",
       "         31: 8,\n",
       "         22: 8,\n",
       "         21: 7,\n",
       "         32: 7,\n",
       "         30: 7,\n",
       "         23: 5,\n",
       "         26: 4,\n",
       "         28: 3,\n",
       "         24: 3,\n",
       "         25: 2,\n",
       "         34: 1,\n",
       "         36: 1,\n",
       "         40: 1,\n",
       "         39: 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for arr in test_predictions.values():\n",
    "    element_counts = Counter(arr)\n",
    "    for key in element_counts.keys():\n",
    "        counter[key] = counter.get(key, 0) + element_counts[key]\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.111905793186735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "res = []\n",
    "for key in test_predictions.keys():\n",
    "    y_pred = test_predictions[key]\n",
    "    y_true = test_true_values[key]\n",
    "\n",
    "    ## mask out the zero values\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    res.append(np.sqrt(mean_squared_error(y_true_filtered, y_pred_filtered)))\n",
    "print(np.mean(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22124508936260653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "res = []\n",
    "for key in test_predictions.keys():\n",
    "    y_pred = test_predictions[key]\n",
    "    y_true = test_true_values[key]\n",
    "\n",
    "    ## mask out the zero values\n",
    "    mask = y_true == 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    res.append(np.sqrt(mean_squared_error(y_true_filtered, y_pred_filtered)))\n",
    "print(np.mean(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "features = ['lon_bin', 'lat_bin', 'date_year', 'day_of_year_sin', 'day_of_year_cos', 'week_of_year_sin', 'week_of_year_cos',\n",
    "            'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'Unemployment_Rate(%)', 'Unemployment_Rank', 'CPI(Annual)']\n",
    "target = 'unique_event_count'\n",
    "\n",
    "loaded_model = xgb.Booster()\n",
    "loaded_model.load_model('model_20240429.xgb')\n",
    "\n",
    "validation_day = df[df['date'] == '2024-04-29']\n",
    "new_data = xgb.DMatrix(validation_day[features])\n",
    "\n",
    "# Prepare new data for prediction\n",
    "# new_data = xgb.DMatrix(new_features)  # `new_features` should be a DataFrame with the same features as the training set\n",
    "new_predictions = loaded_model.predict(new_data)\n",
    "new_predictions_exp = np.exp(new_predictions)  # Back-transform if using exponential in the objective\n",
    "\n",
    "# Optionally round predictions if needed\n",
    "new_predictions_rounded = np.clip(np.round(new_predictions_exp).astype(int), 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions_rounded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crime_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
